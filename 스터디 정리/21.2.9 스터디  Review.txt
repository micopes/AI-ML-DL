파이썬 크롤링으로 가격 비교
pip install selenium

version에 알맞는 크롬 웹드라이버를 다운로드
notepad++를 다운로드
https://youtu.be/dh05lSKt2S4

셀레니움을 이용하면 클릭도 할 수 있고 
클릭하면 또 크롤링하는데 활용할 수 있다.

----------------------------------------------------------------------------------------------
PyTorch란 무엇일까?

Framework
구글브레인-텐서플로우(2015)
페이스북-파이토치(2017)

Tensorflow
- Define and Run 방식, 학습 중에 조건에 따라 변화를 줄 수 없는 정적인 모델.
-> tensorflow2.0에서는 동적인 모델로 변함!!
- 딥러닝 자체에 대한 연구보다 응용 목적으로 실제 산업 현장에서 많이 사용되며 아직도 많은 사용자들이 사용중

PyTorch
- Define by Run, 학습 도중에 단계마다 조절 가능한 동적인 모델.
- 자체 운영 포럼이 있어서 pytorch 개발자들이 직접 답변을 달아준다.

Pytorch의 구성
1. torch
메인 네임 스페이스로 텐서 등의 다양한 수학 함수가 포함되어 있으며, numpy와 유사한 구조를 가집니다.

- tensor: 기본 연산 단위로 numpy array와 유사한 구조를 가지며 numpy->tensor / tensor->numpy가 가능합니다.

2. torch.autograd : 자동 미분
자동 미분을 위한 함수들이 포함되어 있습니다. 자동 미분의 on/off를 제어하는 콘텍스트 매니저(enable_grad/no_grad)나 자체 미분 가능 함수를 정의할 때 사용하는 기반 클래스인 'Function' 등이 포함되어져 있습니다.

3. torch.nn : Layer, Activation Function 등
신경망을 구축하기 위한 다양한 데이터 구조나 레이어 등이 정의되어 있습니다. 예를 들어 RNN, LSTM과 같은 레이어, ReLU와 같은 활성화 함수, MSELoss와 같은 손실 함수들이 있습니다.

- nn은 모델을 정의하고 미분하는데 autograd를 사용하고 nn.Module은 layer와 output을 반환하는 forward(input) 메서드를 포함하고 있습니다.

- nn.Module은 매개변수를 캡슐화하는 간편한 방법으로, GPU로 이동, 내보내기, 불러오기 등의 작업을 위한 헬퍼를 제공합니다.
: 뒷부분은 nn.Module에 구성되어 있고 이를 상속받아서 한다.
- forward 함수만 정의하면 변화도를 계산하는 backward함수는 autograd를 사용하여 자동으로 정의됩니다.

4. torch.optim : 최적화 알고리즘
확률적 경사 하강법(Stochastic Gradient Descent, SGD)을 중심으로 한 파라미터 최적화 알고리즘이 구현되어 있습니다.

5. torch.utils.data : dataloader..
SGD의 반복 연산을 실행할 때 사용하는 미니 배치용 유틸리티 함수가 포함되어 있습니다.

6. torch.onnx
ONNX(Open Neural Network Exchange)의 포맷으로 모델을 익스포트(export)할 때 사용합니다. ONNX는 서로 다른 딥 러닝 프레임워크 간에 모델을 공유할 때 사용하는 포맷입니다.

serving 하는 것은 Pytorch가 약하다.
Pytorch는 Tensorflow를 따라가려고 하고,
Tensorflow는 Pytorch를 따라가려고 한다.

구현은 안하더라도 보고 코드가 어떤 의미인지는 알아야 한다.
---------------------------------------------------------------------------
argparse를 이용해서 hyperparameter를 잘 바꿀 수 있다.

transforms는 어떤 식으로 불러올 것인지 룰을 짜는 것?
dataloader: 비유하자면 휴지곽같은 것. 하나하나 뽑아올 수 있는 것.

Train/Val
중간 단계에서 문제(ex> overfitting)를 확인하기 위해서 validation loss를 이용한다.

.cuda()를 이용해서 GPU이용 가능 한번 .cuda()를 사용했으면 빼먹으면 안된다.

---------------------------------------------------------------------
GoogleNet - Inception Model
DNN에서 Layer는 넓고 깊이가 깊으면 인식률이 좋아진다.
하지만, 깊이가 깊어지는 경우 Vanishing Gradient 문제가 발생

핵심 Idea
1x1 Convolution 
- 깊어지면 내적곱 연산이 늘어나서 파라미터가 기하급수적으로 늘어남.
- 깊게 쌓으면서 파라미터의 개수를 어떻게 조절할 수 있을까?
먼저 filter의 개수를 줄이고 내적곱을 하면 parameter의 개수를 매우 많이 줄일 수 있다!!
-> overfitting 방지, 학습 속도 개선, 

Global Average Pooling
- 맨 마지막의 Flatten(Dense)를 없앤다. 
Average Pooling만으로 classifier 역할을 할 수 있기 때문에 overfitting의 문제를 회피할 수 있다.
(무슨 말일까?)
--------------------------------------------------------------------------
Attention 활용한 기계번역

reduce_sum으로 다른 방식으로 더할 수 있다.
(https://www.tensorflow.org/api_docs/python/tf/math/reduce_sum?version=nightly)

# x has a shape of (2, 3) (two rows and three columns):
x = tf.constant([[1, 1, 1], [1, 1, 1]])
x.numpy()

# sum all the elements
# 1 + 1 + 1 + 1 + 1+ 1 = 6
tf.reduce_sum(x).numpy()

# reduce along the first dimension
# the result is [1, 1, 1] + [1, 1, 1] = [2, 2, 2]
tf.reduce_sum(x, 0).numpy()

# reduce along the second dimension
# the result is [1, 1] + [1, 1] + [1, 1] = [3, 3]
tf.reduce_sum(x, 1).numpy()

# keep the original dimensions
tf.reduce_sum(x, 1, keepdims=True).numpy()


# reduce along both dimensions
# the result is 1 + 1 + 1 + 1 + 1 + 1 = 6
# or, equivalently, reduce along rows, then reduce the resultant array
# [1, 1, 1] + [1, 1, 1] = [2, 2, 2]
# 2 + 2 + 2 = 6
tf.reduce_sum(x, [0, 1]).numpy()

----------------------------------------------------------------------------
kaggle 노트북을 docker로 가져올 수 있다.
단점 : 한글 데이터 전처리 이런 것들이 안깔려있고, 한글 폰트가 안깔려있다는 단점이 있다.



